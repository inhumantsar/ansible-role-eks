
- name: check that aws_iam_authenticator is on the PATH locally
  shell: command -v aws_iam_authenticator >/dev/null 2>&1
  register: aws_iam_authenticator_exists  # var.rc == 0 when binary exists
  ignore_errors: yes
  
- name: install aws-iam-authenticator
  get_url:
    url: "{{ eks_authenticator_url }}"
    checksum: "{{ eks_authenticator_checksum }}"
    dest: /usr/bin/aws-iam-authenticator
    mode: 0755
    owner: root
    group: root
  become: true
  when:
    - aws_iam_authenticator_exists.rc != 0

- name: check that kubectl is on the PATH
  shell: command -v kubectl >/dev/null 2>&1
  register: kubectl_exists  # var.rc == 0 when binary exists
  ignore_errors: yes

- name: ensure kubectl is installed
  get_url:
    url: "{{ eks_kubectl_url }}"
    checksum: "{{ eks_kubectl_checksum }}"
    dest: /usr/bin/kubectl
    mode: 0755
    owner: root
    group: root
  become: true
  when:
    - kubectl_exists.rc != 0

- name: Ensure boto and boto3 modules are installed
  pip:
    name: 
      - boto3
      - botocore


# https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html
# control plane: aws rec. 443 from workers and any bastion host security groups; 1025-65535 to workers
# workers: aws rec. all from workers, 443+1025-65535 from control plane; any to all
- name: create worker security group
  ec2_group:
    name: "{{ eks_worker_sg_name }}"
    description: k8s Workers 
    vpc_id: "{{ eks_cluster_vpc }}"
    region: "{{ eks_cluster_region }}"
    rules: "{{ eks_worker_sg_rules }}"
  register: eks_worker_sg

- name: create control plane security group
  ec2_group:
    name: "{{ eks_control_sg_name }}"
    description: k8s Control Plane 
    vpc_id: "{{ eks_cluster_vpc }}"
    region: "{{ eks_cluster_region }}"
    rules: "{{ eks_control_sg_rules }}"
  register: eks_control_sg
  # of note: sg.group_id  

- name: Create a role with description
  iam_role:
    name: "{{ eks_cluster_name }}-{{ eks_cluster_region }}-eks"
    assume_role_policy_document: "{{ eks_cluster_role_assumerole_policy | to_json }}"
    managed_policy: "{{ eks_cluster_role_managed_policies }}"
    description: Basic role to support an EKS cluster
  register: eks_role

- name: Sleep to give IAM time to propagate changes
  wait_for:
    sleep: 15
  when: eks_role.changed

- name: Create the EKS cluster
  aws_eks_cluster:
    name: "{{ eks_cluster_name }}"
    version: "{{ eks_kubernetes_version }}"
    region: "{{ eks_cluster_region }}"
    role_arn: "{{ eks_role.arn }}"
    subnets: "{{ eks_cluster_subnets }}"
    wait: yes
    security_groups:
      - "{{ eks_control_sg.group_id }}"
      - "{{ eks_worker_sg.group_id }}"
  register: eks_cluster

- name: Create an SSH keypair for worker usage
  ec2_key:
    name: "{{ eks_worker_cluster_name }}"
    region: "{{ eks_cluster_region }}"
    wait: yes

# every EKS cluster can use this same config for general usage.
# probably not worth breaking out into a custom setup. 
# this launches workers into public subnets which i don't think we want...
- name: Launch worker nodes using CloudFormation
  cloudformation:
    stack_name: "{{ eks_worker_cluster_name }}"
    region: "{{ eks_cluster_region }}"
    template_url: "{{ eks_worker_cloudformation_url }}"
    disable_rollback: true
    template_parameters:
      ClusterName: "{{ eks_cluster_name }}"
      ClusterControlPlaneSecurityGroup: "{{ eks_control_sg.group_id }}"
      NodeGroupName: "{{ eks_worker_cluster_name }}"
      NodeAutoScalingGroupMinSize: "{{ eks_worker_min }}"
      NodeAutoScalingGroupDesiredCapacity: "{{ eks_worker_desired_count }}"
      NodeAutoScalingGroupMaxSize: "{{ eks_worker_max }}"
      NodeInstanceType: "{{ eks_worker_node_type }}"
      NodeImageId: "{{ eks_worker_node_ami }}"
      KeyName: "{{ eks_worker_cluster_name }}"
      VpcId: "{{ eks_cluster_vpc }}"
      Subnets: "{{ eks_cluster_subnets | join(',') }}"
  when: "eks_worker_cluster_state == 'present'"
  register: eks_worker_cluster

- debug: var=eks_worker_cluster

- name: set up worker auth
  k8s:
    api_version: v1
    kind: ConfigMap
    resource_definition:
      metadata:
        name: aws-auth
        namespace: kube-system
      data:
        mapRoles: |
          - rolearn: "{{ eks_worker_cluster.stack_outputs.NodeInstanceRole }}"
            username: "{{ eks_worker_auth_username }}"
            groups:
              - system:bootstrappers
              - system:nodes

# # at least one storage class definition is recommended for persistent volumes
# - name: Create Slow storage class, backed by AWS EBS
#   k8s:
#     definition:
#       apiVersion: storage.k8s.io/v1
#       kind: StorageClass
#       metadata:
#         name: ebs-hdd
#       provisioner: kubernetes.io/aws-ebs
#       parameters:
#         type: io1
#         fsType: ext4    

# - name: Create Fast storage class, backed by AWS EBS
#   k8s:
#     definition:
#       apiVersion: storage.k8s.io/v1
#       kind: StorageClass
#       metadata:
#         name: ebs-gp2
#       provisioner: kubernetes.io/aws-ebs
#       parameters:
#         type: gp2
#         fsType: ext4    



# - route53:
#       state: present
#       zone: foo.com
#       record: my_cluster.foo.com
#       type: CNAME
#       value: {{ caller_facts.endpoint }}
  


